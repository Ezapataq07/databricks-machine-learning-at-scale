{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904fccba-6822-463e-8df2-0832f33eeddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS workspace.ml_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9ef5ba9-3c16-4f21-9a49-c030ebbcd41f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Before we save features to a feature table we need to create features that we are interested in. Feature\n",
    "selection criteria depend on your project goals and business problem. Thus, in this section, we will pick some\n",
    "features, however, it doesn't necessarily mean that these features are significant for our purpose.\n",
    "\n",
    "**One important point is that you need to exclude the target field from the feature table and you need to\n",
    "define a primary key for the table.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d673b69d-836a-420d-9a41-2805f0279358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "Typically, first, you will need to conduct data pre-processing and select features. As we covered data pre-\n",
    "processing and feature preparation, we will load a clean dataset which you would typically load from a\n",
    "`silver` table.\n",
    "\n",
    "Let's load in our dataset from a CSV file containing Telco customer churn data from the specified path using\n",
    "Apache Spark.** In this dataset the target column will be Churn and primary key will be customerID .**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0d2a78e-0b78-484c-ba90-fdf63d3e0b16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql('select * from samples.tpch.customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f441679-969c-4b60-bc9f-6f7e49e4df10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a13483a-803f-4ac8-9b9e-1a3c1789cb04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save Features to Feature Table\n",
    "\n",
    "Let's start creating a Feature Engineering Client so we can populate our feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac404c4d-7bda-44f9-bc5e-00f6ecdefa74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature-engineering\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cacaa6f-2568-446d-a4e4-da3f69f01225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "help(fe.create_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad7af13f-661f-41e1-94d4-58caa2b99c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Feature Table\n",
    "\n",
    "Next, we can create the Feature Table using the create_table method.\n",
    "\n",
    "This method takes a few parameters as inputs:\n",
    ". `name` - A feature table name of the form` <catalog> .< schema_name>. <table_name>`\n",
    "\n",
    ". `primary_keys` - The primary key(s). If multiple columns are required, specify a list of column names.\n",
    "\n",
    ". `timestamp_col` - [OPTIONAL] any timestamp column which can be used for point-in-time lookup.\n",
    "\n",
    ". `df` - Data to insert into this feature table. The schema of features_df will be used as the feature table\n",
    "schema.\n",
    "\n",
    ". `schema` - Feature table schema. Note that either schema or features_df must be provided.\n",
    "\n",
    "· `description` - Description of the feature table\n",
    "\n",
    "· `partition_columns` - Column(s) used to partition the feature table.\n",
    "\n",
    "· `tags` - Tag(s) to tag feature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e07eb9c3-8cec-4978-9cce-6ca1cea24b5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = 'workspace.ml_training.features_customer_tpch'\n",
    "\n",
    "fe.create_table(\n",
    "    name = table_name,\n",
    "    primary_keys=['c_custkey'],\n",
    "    df = df,\n",
    "    # partition_columns=\n",
    "    description = 'TPCH Customers Features',\n",
    "    tags = {'source':'samples', 'format':'delta'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48a9ba08-0a4c-45c7-8362-e98370813cd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Alternatively, you can create_table with schema only (without df), and populate data to the feature table\n",
    "with fe.write_table. fe.write_table has merge mode ONLY (to overwrite, we should drop and then re-\n",
    "create the table).\n",
    "\n",
    "Example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e55ae0af-6296-40b4-af92-7be5b9f3da7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  One time creation\n",
    "# fs.create_table(\n",
    "#     name=table_name,\n",
    "#     primary_keys=[\"index\"],\n",
    "#     schema=telco_df.schema,\n",
    "#     description=\"Original Telco data (Silver)\"\n",
    "# )\n",
    "\n",
    "#  Repeated/Scheduled writes\n",
    "# fs.write_table(\n",
    "#     name=table_name,\n",
    "#     df=telco_df,\n",
    "#     mode=\"merge\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e7afbc4-f5a1-420b-ac40-822570d3676e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Feature Table\n",
    "\n",
    "We can also look at the metadata of the feature store via the FeatureStore client by using `get_table()` . As\n",
    "feature table is a Delta table we can load it with Spark as normally we do for other tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bd86f7b-b483-474d-a8d5-70fc4f3a48e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ft = fe.get_table(name=table_name)\n",
    "print(f'Feature Table description: {ft.description}')\n",
    "print(ft.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234be674-720e-49bb-9424-650b462d65c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fe.read_table(name=table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45717d66-a116-4b34-a018-b2c5dc298e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Update Feature Table\n",
    "\n",
    "In some cases we might need to update an existing feature table by adding new features or deleting existing\n",
    "features. In this section, we will show to make these type of changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf7e898a-5008-49b3-8c8b-8d6e55355748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add a New Feature\n",
    "\n",
    "To illustrate adding a new feature, let's redefine an existing one. In this case, we'll transform the tenure\n",
    "column by categorizing it into three groups: `short`, `mid`, and `long` , representing different tenure durations.\n",
    "\n",
    "Then we will write the dataset back to the feature table. The important parameter is the mode parameter,\n",
    "which we should set to \"`merge`\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c8153b-86b1-4009-a1e9-8fb5e73da5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "df_updated = df.withColumn('c_acctbal_group', f.when(\n",
    "    f.col('c_acctbal') >= 5000 , \"long\"\n",
    ").otherwise(\n",
    "    \"short\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e70d6ee4-a478-490b-9634-22fca2259a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Selecting relevant columns. Use an appropriate mode (e.g., \"merge\") and display the written table for\n",
    "validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d1f6c75-8217-41fd-afd1-915cda64e703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe.write_table(\n",
    "  name=table_name,\n",
    "  df=df_updated.select('c_custkey', 'c_acctbal_group'),\n",
    "  mode='merge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "012d480f-3371-4223-a257-f6867e220d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Delete Existing Feature\n",
    "\n",
    "To remove a feature column from the table you can just drop the column. Let's drop the original tenure\n",
    "column.\n",
    "\n",
    "**Note:** We need to set Delta read and write protocal version manually to support column mapping. If you\n",
    "want to learn more about this you can check related documentation page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9410af1-9262-416b-b01c-0b9d9e166b6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "ALTER TABLE workspace.ml_training.features_customer_tpch SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.minReaderVersion'='2', 'delta.minWriterVersion'='5');\n",
    "ALTER TABLE workspace.ml_training.features_customer_tpch DROP COLUMNS (c_acctbal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c19a148-bc0c-481c-bda2-af7a92b54f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read Feature Table by Version\n",
    "\n",
    "As feature tables are based on Delta tables, we get all nice features of Delta including versioning. To\n",
    "demonstrate this, let's read from a snapshot of the feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7613e4-9779-4f97-951d-06e64012aa08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timestamp_v3 = spark.sql(f'DESCRIBE HISTORY {table_name}').orderBy('version').collect()[2].timestamp\n",
    "print(timestamp_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb4a2c79-e79a-4a9e-b65b-74c2897a8130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_v3 = (\n",
    "  spark\n",
    "  .read\n",
    "  .option('timestampAsOf', timestamp_v3)\n",
    "  .table(table_name)\n",
    ")\n",
    "\n",
    "display(df_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09ef946a-0073-4bd3-a046-62735b301f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_df_v3 = fe.read_table(name=table_name, as_of_delta_timestamp=timestamp_v3)\n",
    "feature_df_v3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a4998fb-976f-45e9-9e4f-f1ea2a41e4de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a Feature Table from Existing UC Table\n",
    "\n",
    "Alter/Change existing UC table to become a feature table Add a primary key (PK) with non-null constraint\n",
    "(with timestamp if applicable) on any UC table to turn it into a feature table (more info here)\n",
    "\n",
    "In this example, we have a table created in the beginning of the demo which contains security features. Let's\n",
    "convert this delta table to a feature table.\n",
    "\n",
    "For this, we need to do these two changes;\n",
    "\n",
    "1. Set primary key columns to NOT NULL .\n",
    "\n",
    "2. Alter the table to add the Primary Key Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b43ea6f-5f38-4bdb-a332-c6ae3775ef1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parts = spark.sql('select * from samples.tpch.part')\n",
    "df_parts.write.format('delta').mode('overwrite').saveAsTable('workspace.ml_training.features_parts')\n",
    "display(df_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f51dee4-b174-4926-a7ad-241627aaaf49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE workspace.ml_training.features_parts ALTER COLUMN p_partkey SET NOT NULL;\n",
    "ALTER TABLE workspace.ml_training.features_parts ADD CONSTRAINT parts_features_pk_constraint PRIMARY KEY (p_partkey);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0790476e-9307-4248-a2b2-01f6c4859ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60b90c0b-db80-4119-822a-cf5113810834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7735286e-837e-4ebd-abc7-eed01dbba619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9cbb766-9731-461e-9730-f75b68bcb63d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8325480935519407,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "FeatureStore",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
